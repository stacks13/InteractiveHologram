{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import socketio\n",
    "import queue\n",
    "import threading\n",
    "import os\n",
    "import json\n",
    "from numba import jit, prange\n",
    "from numba.typed import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for 21 points on the hand\n",
    "HAND_POINT_COLORS = [\n",
    "    (100, 100, 100), (100, 0, 0), (150, 0, 0), (200, 0, 0), (255, 0, 0),\n",
    "    (100, 100, 0), (150, 150, 0), (200, 200, 0), (255, 255, 0), (0, 100, 50),\n",
    "    (0, 150, 75), (0, 200, 100), (0, 255, 125), (0, 50, 100), (0, 75, 150),\n",
    "    (0, 100, 200), (0, 125, 255), (100, 0, 100), (150, 0, 150), (200, 0, 200),\n",
    "    (255, 0, 255),\n",
    "]\n",
    "TRAIN_IMAGE_HEIGHT = 256\n",
    "TRAIN_IMAGE_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, cache=True)\n",
    "def is_local_value_great(src, locval, temploc):\n",
    "    blockwidth=2\n",
    "    startout = max(temploc[0] - blockwidth, 0)\n",
    "    endout = min(temploc[0] + blockwidth, src.shape[1] - 1) + 1\n",
    "    startin = max(temploc[1] - blockwidth, 0)\n",
    "    endin = min(temploc[1] + blockwidth, src.shape[0] - 1) + 1\n",
    "    \n",
    "    for m in prange(startout, endout):\n",
    "        for n in prange(startin, endin):\n",
    "            if float(src[n, m]) > locval:\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True, cache=True)\n",
    "def get_locations(src, threshold):\n",
    "    location = List()\n",
    "    blockwidth=2\n",
    "\n",
    "    endout = src.shape[1] - blockwidth\n",
    "    endin = src.shape[0] - blockwidth\n",
    "    \n",
    "    for i in prange(blockwidth, endout):\n",
    "        for j in prange(blockwidth, endin):\n",
    "            tmploc = (i, j)\n",
    "            localvalue = float(src[j, i])\n",
    "            \n",
    "            if localvalue < threshold:\n",
    "                continue\n",
    "                \n",
    "            localmaximum = is_local_value_great(src, localvalue, tmploc)\n",
    "            \n",
    "            if localmaximum:\n",
    "                location.append((localvalue, np.array(tmploc)))\n",
    "                \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(cache=True, forceobj=True, nopython=True)\n",
    "def transform_input(input_tensor, src_img, tensor_index=0):\n",
    "    ratio = min(\n",
    "        input_tensor.shape[2] / src_img.shape[0],\n",
    "        input_tensor.shape[3] / src_img.shape[1]\n",
    "    )\n",
    "    M = np.float32([[ratio, 0, 0], [0, ratio, 0]])\n",
    "    dst = cv2.warpAffine(src_img, M, (input_tensor.size(3), input_tensor.size(2)), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=128)\n",
    "    \n",
    "    dst = dst / 255.0 - 0.5\n",
    "    chn_img = cv2.split(dst)\n",
    "    \n",
    "    if len(chn_img) == 1:\n",
    "        input_tensor[tensor_index][0] = torch.from_numpy(chn_img[0])\n",
    "        input_tensor[tensor_index][1] = torch.from_numpy(chn_img[0])\n",
    "        input_tensor[tensor_index][2] = torch.from_numpy(chn_img[0])\n",
    "    else:\n",
    "        input_tensor[tensor_index][0] = torch.from_numpy(chn_img[0])\n",
    "        input_tensor[tensor_index][1] = torch.from_numpy(chn_img[1])\n",
    "        input_tensor[tensor_index][2] = torch.from_numpy(chn_img[2])\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_bounding_box(boxes, model, input_image):\n",
    "    input_tensor = torch.zeros(1, 3, TRAIN_IMAGE_HEIGHT, TRAIN_IMAGE_WIDTH)\n",
    "    \n",
    "    ratio_input_to_net = transform_input(input_tensor, input_image)\n",
    "    \n",
    "    heatmap = model.forward(input_tensor.cuda())[3]\n",
    "    ratio_net_downsample = TRAIN_IMAGE_HEIGHT / heatmap.size(2)\n",
    "    rect_map_idx = heatmap.size(1) - 3\n",
    "    rectmap = [None] * 3\n",
    "    \n",
    "    for i in range(3):\n",
    "        rectmap[i] = heatmap[0][i + rect_map_idx].cpu().detach().numpy()\n",
    "    \n",
    "    locations = get_locations(rectmap[0], HAND_THRESHOLD)\n",
    "\n",
    "    boxes.clear()\n",
    "    \n",
    "    for i in locations:\n",
    "        points = i[1]\n",
    "        pos_x = points[0]\n",
    "        pos_y = points[1]\n",
    "        \n",
    "        ratio_width = 0.0\n",
    "        ratio_height = 0.0\n",
    "        pixelcount = 0\n",
    "        \n",
    "        for m in range(max(pos_y - 2, 0), min(pos_y + 3, heatmap.size(2))):\n",
    "            for n in range(max(pos_x - 2, 0), min(pos_x + 3, heatmap.size(3))):\n",
    "                ratio_width += float(rectmap[1][m, n])\n",
    "                ratio_height += float(rectmap[2][m, n])\n",
    "                pixelcount += 1\n",
    "                \n",
    "        if pixelcount > 0:\n",
    "            ratio_width = min(max(ratio_width/pixelcount, 0.0), 1.0)\n",
    "            ratio_height = min(max(ratio_height/pixelcount, 0.0), 1.0)\n",
    "            \n",
    "            points = points * ratio_net_downsample / ratio_input_to_net\n",
    "            rect_w = ratio_width * TRAIN_IMAGE_WIDTH / ratio_input_to_net\n",
    "            rect_h = ratio_height * TRAIN_IMAGE_HEIGHT / ratio_input_to_net\n",
    "            \n",
    "            lt = (points - np.array([rect_w//2, rect_h//2])).astype('int32')\n",
    "            rb = (points + np.array([rect_w//2, rect_h//2])).astype('int32')\n",
    "            lt[0] = max(lt[0], 0)\n",
    "            lt[1] = max(lt[1], 0)\n",
    "            rb[0] = min(rb[0], input_image.shape[1]-1)\n",
    "            rb[1] = min(rb[1], input_image.shape[0]-1)\n",
    "            boxes.append((lt[0], lt[1], rb[0]-lt[0], rb[1]-lt[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hand_points(manypoints, model, input_image, boxes, locs, rects):\n",
    "    input_tensor = torch.zeros(len(boxes), 3, TRAIN_IMAGE_HEIGHT, TRAIN_IMAGE_WIDTH)\n",
    "    ratio_input_to_net = [None]*len(boxes)\n",
    "    for i in range(len(boxes)):\n",
    "        a = int(boxes[i][0])\n",
    "        b = a + int(boxes[i][2])\n",
    "        c = int(boxes[i][1])\n",
    "        d = c + int(boxes[i][3])\n",
    "        sliced = input_image[c:d, a:b]\n",
    "        ratio_input_to_net[i] = transform_input(input_tensor, sliced, i)\n",
    "    \n",
    "    net_result = model.forward(input_tensor.cuda())[3]\n",
    "    ratio_net_downsample = TRAIN_IMAGE_HEIGHT / net_result.size(2)\n",
    "    \n",
    "    if len(boxes) == 2:\n",
    "        rects.clear()\n",
    "    \n",
    "    for rect_idx in range(len(boxes)):\n",
    "        rect = []\n",
    "        for i in range(net_result.size(1) - 3):\n",
    "            heatmap = net_result[rect_idx][i].cpu().detach().numpy()\n",
    "            pts = get_locations(heatmap, FINGER_THRESHOLD)\n",
    "\n",
    "            count = 0\n",
    "            for j in pts:\n",
    "                if count >= 1:\n",
    "                    break\n",
    "                temp = j[1]\n",
    "                \n",
    "                if count == 0:\n",
    "                    locs[i] = j[1].copy()\n",
    "                \n",
    "                temp = temp*ratio_net_downsample/ratio_input_to_net[rect_idx]\n",
    "                temp = temp + np.array([boxes[rect_idx][0], boxes[rect_idx][1]])\n",
    "                manypoints[i].append((j[0], temp))\n",
    "                rect.append(temp.copy())\n",
    "                count += 1\n",
    "                \n",
    "        rects.append(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(forceobj=True)\n",
    "def detect_box_and_hand(model, input_image):\n",
    "    center = np.array([0, 0])\n",
    "    rects = []\n",
    "    boxes = []\n",
    "    locs = [None]*21\n",
    "\n",
    "    many_keypoints = [[] for _ in range(21)]\n",
    "\n",
    "    boxes.append((0, 0, input_image.shape[1], input_image.shape[0]))\n",
    "    detect_bounding_box(boxes, model, input_image)\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        return (locs, [], None)\n",
    "\n",
    "    thickness_circle_ratio = 1 / 120\n",
    "    thickness_circle = max(int(math.sqrt(input_image.shape[1] * input_image.shape[0]) * thickness_circle_ratio + 0.5), 2)\n",
    "    number_colors = len(HAND_POINT_COLORS)\n",
    "    \n",
    "    detect_hand_points(many_keypoints, model, input_image, boxes, locs, rects)\n",
    "    count = 0\n",
    "    for curr_joint_index in range(0, 21):\n",
    "        curr_color = HAND_POINT_COLORS[curr_joint_index % number_colors]\n",
    "        \n",
    "        for i in many_keypoints[curr_joint_index]:\n",
    "            center = center + i[1]\n",
    "            count += 1\n",
    "            cv2.circle(input_image, (int(i[1][0]), int(i[1][1])), thickness_circle, curr_color, -1)\n",
    "#             cv2.putText(input_image, str(curr_joint_index), (int(i[1][0]), int(i[1][1])), cv2.FONT_HERSHEY_SIMPLEX, 0.25, (0, 0, 255), 1)\n",
    "    \n",
    "    if count != 0:\n",
    "        center = center / count\n",
    "#         cv2.circle(input_image, (int(centers[0]), int(centers[1])), thickness_circle, (0, 0, 0), -1)\n",
    "        \n",
    "    for i in boxes:\n",
    "        cv2.rectangle(input_image, i, (0, 255, 0), thickness=int(min(input_image.shape[1] / TRAIN_IMAGE_WIDTH * 3.0, 3.0)))\n",
    "\n",
    "    return (locs, rects, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_points(points):\n",
    "    temp = [20, 16, 12, 8, 17, 13, 9, 5]\n",
    "    for i in range(len(temp)):\n",
    "        if points[temp[i]] is None:\n",
    "            points[temp[i]] = points[temp[(i+4)%8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = torch.jit.load('hand.pts', map_location=torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    '1_finger',\n",
    "    '2_finger',\n",
    "    '3_finger',\n",
    "    '4_finger',\n",
    "    '5_finger',\n",
    "    'okay',\n",
    "    'thumbs_up',\n",
    "    'thumbs_down',\n",
    "    'call',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio1 = socketio.Client()\n",
    "\n",
    "@sio1.event\n",
    "def connect():\n",
    "    print('connection established')\n",
    "    sio1.emit('model')\n",
    "\n",
    "@sio1.event\n",
    "def disconnect():\n",
    "    print('disconnected from server')\n",
    "    \n",
    "@sio1.event\n",
    "def predictions(data):\n",
    "    global prediction\n",
    "    prediction = np.array(data['preds'])\n",
    "\n",
    "# run cell only if sio_server.py is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection established\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "%matplotlib qt\n",
    "\n",
    "model_2_input = None\n",
    "prediction = None\n",
    "payload = {}\n",
    "\n",
    "gestures_history = deque(maxlen=5)\n",
    "centers_history = deque(maxlen=10)\n",
    "distances_history = deque(maxlen=2)\n",
    "mode = 0\n",
    "\n",
    "HAND_THRESHOLD = 0.8\n",
    "FINGER_THRESHOLD = 0.2\n",
    "\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "sio1.connect('http://127.0.0.1:8080')\n",
    "while True:\n",
    "\n",
    "    ret, frame = capture.read()\n",
    "    \n",
    "    if frame is None:\n",
    "        print(\"Cannot open camera\")\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "    t = cv2.getTickCount()\n",
    "\n",
    "    try:\n",
    "        points, rects, center  = detect_box_and_hand(model_1, frame)\n",
    "    except OverflowError:\n",
    "        print('overflow error')\n",
    "        continue\n",
    "    \n",
    "    no_of_pts = len([i for i in points if i is not None])\n",
    "    \n",
    "    \n",
    "    # if no. of points detected are enough, add input for model2 to payload\n",
    "    if 21-no_of_pts < 5:\n",
    "        clean_points(points)\n",
    "        if 21-no_of_pts == 0:\n",
    "            model2_input = np.array(points).flatten()\n",
    "            payload.update({'m2': model2_input.tolist()})\n",
    "        else:\n",
    "            for i in range(len(points)):\n",
    "                if points[i] is None:\n",
    "                    points[i] = np.mean([j for j in points if j is not None], axis=0)\n",
    "            model2_input = np.array(points).flatten()\n",
    "            payload.update({'m2': model2_input.tolist()})\n",
    "#             model2_input = None\n",
    "    \n",
    "    # if two bounding boxes exist, add distances between their centers to distance history\n",
    "    # else clear distance history\n",
    "    if len(rects) == 2:\n",
    "        if len(distances_history) == 2:\n",
    "            distances_history.append(np.linalg.norm(\n",
    "                np.mean(rects[0], axis=0) - np.mean(rects[1], axis=0)\n",
    "            ))\n",
    "            z = round(distances_history[-1]-distances_history[-2], 2)\n",
    "            payload.update({'z': float(z)})\n",
    "        else:\n",
    "            distances_history.append(np.linalg.norm(\n",
    "                np.mean(rects[0], axis=0) - np.mean(rects[1], axis=0)\n",
    "            ))\n",
    "    else:\n",
    "        distances_history.clear()\n",
    "    \n",
    "        \n",
    "    # add current center to history if it is not None, if none clear history\n",
    "    if center is not None:\n",
    "        centers_history.append(center)\n",
    "        \n",
    "        # add difference between centers to payload if current center is detected\n",
    "        if len(centers_history) >= 2 and center is not None:        \n",
    "            c = centers_history[-1]-centers_history[-2]\n",
    "            c[0] = round(c[0], 1)\n",
    "            c[1] = round(c[1], 1)\n",
    "\n",
    "            payload.update({'x': float(c[0]), 'y': float(c[1])})\n",
    "\n",
    "            # for quick rotate mode, add direction of rotation to payload\n",
    "            if mode == 5 and len(centers_history) == 10:\n",
    "                diff_pt = np.array([0, 0], dtype='float64')\n",
    "                for i in range(1, len(centers_history)):\n",
    "                    diff_pt += (centers_history[i-1] - centers_history[i])\n",
    "\n",
    "                direction = 'l' if diff_pt[0] > 0 else 'r'\n",
    "                if abs(diff_pt[0]) > 30:\n",
    "                    payload.update({'rotate': direction})\n",
    "                    centers_history.clear()\n",
    "\n",
    "    else:\n",
    "        centers_history.clear()\n",
    "                     \n",
    "    # add current prediction to payload for stone paper sicssors, plot prediction graph\n",
    "    if prediction is not None:\n",
    "        payload.update({'pred': labels[np.argmax(prediction[0])]})\n",
    "        plt.cla()\n",
    "        plt.bar(labels, prediction[0], 0.2)\n",
    "        plt.ylim([0, 1])\n",
    "        plt.draw()\n",
    "\n",
    "    \n",
    "    # change current mode based on past gesture, clear gesture history if no gesture is detected\n",
    "    if prediction is not None and prediction[0].max() > 0.4:\n",
    "        label = labels[np.argmax(prediction[0])]\n",
    "\n",
    "        gestures_history.append(label)\n",
    "        if len(gestures_history) == 5 and len(set(gestures_history)) == 1:\n",
    "            if label == 'okay':\n",
    "                mode = 0\n",
    "            elif label == '5_finger':\n",
    "                pass\n",
    "            elif label == '1_finger':\n",
    "                mode = 1\n",
    "            elif label == '2_finger':\n",
    "                mode = 2\n",
    "            elif label == '3_finger':\n",
    "                mode = 3\n",
    "            elif label == 'thumbs_up':\n",
    "                mode = 4\n",
    "            elif label == 'thumbs_down':\n",
    "                mode = 5\n",
    "                centers_history.clear()\n",
    "\n",
    "        cv2.putText(frame, label, (frame.shape[1]-500, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "        prediction = None\n",
    "    else:\n",
    "        gestures_history.clear()\n",
    "            \n",
    "    # calculate fps\n",
    "    t = (cv2.getTickCount() - t) / cv2.getTickFrequency()\n",
    "    fps = 1.0 / t\n",
    "    \n",
    "    cv2.putText(frame, \"FPS: {}\".format(round(fps, 2)), (frame.shape[1] - 200, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    cv2.putText(frame, \"PTS: {}\".format(no_of_pts), (frame.shape[1] - 200, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "    cv2.imshow('hand detection', frame)\n",
    "    \n",
    "    # send payload to server\n",
    "    payload.update({'mode': mode})\n",
    "    sio1.emit('transmit', payload)\n",
    "    payload.clear()\n",
    "    \n",
    "    if cv2.waitKey(1) == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "capture.release()\n",
    "plt.close()\n",
    "cv2.destroyAllWindows()\n",
    "sio1.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio1.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
